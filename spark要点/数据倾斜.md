在执行shuffle操作的时候，是按照key，来进行values的数据的输出、拉取和聚合的。
同一个key的values，一定是分配到一个reduce task进行处理的。
多个key对应的values，总共是90万。但是问题是，可能某个key对应了88万数据，key-88万values，分配到一个task上去面去执行。
另外两个task，可能各分配到了1万数据，可能是数百个key，对应的1万条数据。


spark数据倾斜，有两种表现：
    1.你的大部分的task，都执行的特别特别快就执行完了,剩下几个task，执行的特别特别慢

    2.运行的时候，其他task都执行完了，也没什么特别的问题；但是有的task，就是会突然间报了一个OOM，内存溢出了，task failed，task lost，resubmitting task。反复执行几次都到了某个task就是跑不通，最后就挂掉。
    某个task就直接OOM，那么基本上也是因为数据倾斜了，task分配的数量实在是太大了！！！所以内存放不下，然后你的task每处理一条数据，还要创建大量的对象。内存爆掉了。

    作业都跑不完，还谈什么性能调优这些东西。扯淡。。。



定位原因与出现问题的位置:
    出现数据倾斜的原因，基本只可能是因为发生了shuffle操作，在shuffle的过程中，出现了数据倾斜的问题。因为某个，或者某些key对应的数据，远远的高于其他的key。

1.你在自己的程序里面找找，哪些地方用了会产生shuffle的算子，groupByKey、countByKey、reduceByKey、join
2.看log,log一般会报是在你的哪一行代码，导致了OOM异常；或者呢，看log，看看是执行到了第几个stage！！！




解决方案一共有7个：
1.聚合源数据
    1.将key对应的所有的values，在hive etl中全部用一种特殊的格式，拼接到一个字符串里面去
    2.把相同key，按照不同粒度聚合


2.过滤导致倾斜的key
如果能接受过滤掉少量导致倾斜的key，在查询时直接在hive中用where过滤掉，再形成RDD


3.提高shuffle 操作reduce并行度
将reduce task的数量，变多，就可以让每个reduce task分配到更少的数据量，这样的话，也许就可以缓解，或者甚至是基本解决掉数据倾斜的问题。

    如何操作？
    比如groupByKey、countByKey、reduceByKey。在调用的时候，传入进去一个参数。一个数字。那个数字，就代表了那个shuffle操作的reduce端的并行度。
    这样的话，就可以让每个reduce task分配到更少的数据。基本可以缓解数据倾斜的问题。

    这种方法治标不治本，它没有从根本上改变数据倾斜的本质和问题

    实际生产环境中的经验。

    1、如果最理想的情况下，提升并行度以后，减轻了数据倾斜的问题，或者甚至可以让数据倾斜的现象忽略不计，那么就最好。就不用做其他的数据倾斜解决方案了。

    2.有效果，不明显，就立即放弃第三种方案，开始去尝试和选择后面的四种方案。



4.使用随机key实现双重聚合
    使用场景
    （1）groupByKey
    （2）reduceByKey
    join，咱们通常不会这样来做，

    第一轮聚合的时候，对key进行打散，将原先一样的key，变成不一样的key，相当于是将每个key分为多组；

    先针对多个组，进行key的局部聚合；接着，再去除掉每个key的前缀，然后对所有的key，进行全局的聚合。

    对groupByKey、reduceByKey造成的数据倾斜，有比较好的效果。

    如果说，之前的第一、第二、第三种方案，都没法解决数据倾斜的问题，那么就只能依靠这一种方式了。


5.将reduce join转换为map join
    reduce join转换为map join，适合在什么样的情况下，可以来使用？
    如果两个RDD要进行join，其中一个RDD是比较小的。一个RDD是100万数据，一个RDD是1万数据。（一个RDD是1亿数据，一个RDD是100万数据）
    其中一个RDD必须是比较小的，broadcast出去那个小RDD的数据以后，就会在每个executor的block manager中都驻留一份。要确保你的内存足够存放那个小RDD中的数据
    这种方式下，根本不会发生shuffle操作，肯定也不会发生数据倾斜；从根本上杜绝了join操作可能导致的数据倾斜的问题；
    对于join中有数据倾斜的情况，大家尽量第一时间先考虑这种方式，效果非常好；如果某个RDD比较小的情况下。


    不适合的情况：
    两个RDD都比较大，那么这个时候，你去将其中一个RDD做成broadcast，就很笨拙了。很可能导致内存不足。最终导致内存溢出，程序挂掉。
    而且其中某些key（或者是某个key），还发生了数据倾斜；此时可以采用最后两种方式。

    使用map join的方式，牺牲一点内存资源；在可行的情况下，优先这么使用。

    不走shuffle，直接走map，是不是性能也会高很多？这是肯定的。


6.sample采样倾斜key进行两次join
    其实关键之处在于，将发生数据倾斜的key，单独拉出来，放到一个RDD中去；就用这个原本会倾斜的key RDD跟其他RDD，单独去join一下，这个时候，key对应的数据，可能就会分散到多个task中去进行join操作。

    就不至于说是，这个key跟之前其他的key混合在一个RDD中时，肯定是会导致一个key对应的所有数据，都到一个task中去，就会导致数据倾斜。

    这种方案什么时候适合使用？

    优先对于join，肯定是希望能够采用上一讲讲的，reduce join转换map join。两个RDD数据都比较大，那么就不要那么搞了。

    针对你的RDD的数据，你可以自己把它转换成一个中间表，或者是直接用countByKey()的方式，你可以看一下这个RDD各个key对应的数据量；此时如果你发现整个RDD就一个，或者少数几个key，是对应的数据量特别多；尽量建议，比如就是一个key对应的数据量特别多。

    此时可以采用咱们的这种方案，单拉出来那个最多的key；单独进行join，尽可能地将key分散到各个task上去进行join操作。

    什么时候不适用呢？

    如果一个RDD中，导致数据倾斜的key，特别多；那么此时，最好还是不要这样了；还是使用我们最后一个方案，终极的join数据倾斜的解决方案。



7.使用随机数以及扩容表进行join
步骤：
1、选择一个RDD，要用flatMap，进行扩容，将每条数据，映射为多条数据，每个映射出来的数据，都带了一个n以内的随机数，通常来说，会选择10。
2、将另外一个RDD，做普通的map映射操作，每条数据，都打上一个10以内的随机数。
3、最后，将两个处理后的RDD，进行join操作。


局限性：
1、因为你的两个RDD都很大，所以你没有办法去将某一个RDD扩的特别大，一般咱们就是10倍。
2、如果就是10倍的话，那么数据倾斜问题，的确是只能说是缓解和减轻，不能说彻底解决。

sample采样倾斜key并单独进行join

将key，从另外一个RDD中过滤出的数据，可能只有一条，或者几条，此时，咱们可以任意进行扩容，扩成1000倍。

将从第一个RDD中拆分出来的那个倾斜key RDD，打上1000以内的一个随机数。

这种情况下，还可以配合上，提升shuffle reduce并行度，join(rdd, 1000)。通常情况下，效果还是非常不错的。

打散成100份，甚至1000份，2000份，去进行join，那么就肯定没有数据倾斜的问题了吧。
