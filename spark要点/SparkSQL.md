1.为什么要有sparkSQL？
Hive是为了不会MapReduce的人能够使用SQL，但是底层基于MapReduce，MapReduce基于磁盘，因此HIVE性能很不好，sparkSQL基于内存计算模型

内存列存储，不是使用java对象的方式来存储，而是使用面向列的内存存储方式，也就是，每一列，作为一个数据存储的单位，大大优化了内存使用效率，减少了对内存的消耗，避免了gc大量数据的性能开销。


2.为什么要将RDD转换为DataFrame？
可以针对任何可以转化为RDD的数据进行SQL查询
SparkSQL支持两种方式，将RDD转换为DataFrame
1.使用反射，当已经知道RDD元数据时是不错的方式
2.通过编程接口来创建DataFrame，运行程序时动态构建一份元数据，然后将其应用在已经存在的RDD上。这种方式，如果在编写代码时，还不知道原数组，只有在程序运行的时候，才能动态得知其元数据，那么只能通过这种动态构建元数据的方式

