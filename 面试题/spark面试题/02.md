1.在Apache Hadoop上运行Spark有哪些不同的方法？
我们可以在Hadoop生态系统上使用spark -spark with HDFS 你可以在HDFS中读取和写入数据 -spark with Hive 你可以读取和分析并写回到hive

2.Apache Spark中的SparkContext是什么？
SparkContext是Spark执行环境的客户端，它充当Spark应用程序的主服务器。
SparkContext设置内部服务并建立与Spark执行环境的连接。
每个JVM只能激活一个SparkContext。您必须在创建新的SparkContext之前停止（）活动的SparkContext。

3.Apache Spark中的SparkSession是什么？
从Apache Spark 2.0开始，Spark Session是Spark应用程序的新入口点。
在2.0之前，SparkContext是spark工作的切入点。
Spark Session还包括不同上下文中可用的所有API - Spark Context，SQL Context，Streaming Context，Hive Context。

4.默认情况下，Apache Spark中的RDD中创建了多少个分区？
默认情况下，Spark为文件的每个块创建一个分区（对于HDFS）
HDFS块的默认块大小为64 MB（Hadoop版本1）/ 128 MB（Hadoop版本2）。
但是，可以明确指定要创建的分区数。传入参数 sc.textfile

5.scala 语言有什么特点，相比java有什么优点?
scala使用java语言编写的，代码量很少

6.什么是Scala的伴生类和伴生对象?
单例对象与类同名时，这个单例对象被称为这个类的伴生对象，而这个类被称为这个单例对象的伴生类。伴生类和伴生对象要在同一个源文件中定义，伴生对象和伴生类可以互相访问其私有成员。不与伴生类同名的单例对象称为孤立对象。

