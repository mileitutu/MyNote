第一个模块的简单总结：

1、完整的大数据项目开发流程：数据分析、需求分析、技术方案设计、数据表设计、代码编写、功能测试、性能调优、（上线）troubleshooting、（上线）解决数据倾斜问题。
2、交互式大数据分析系统的架构：J2EE+Spark；
3、基础组件：企业级大数据项目，spark工程，架构
4、复杂的用户分析的业务：聚合统计、按时间比例随机抽取、复杂排序、取topn、用户行为分析
5、spark的各种算子：map、reduce、join、group
6、spark的高级技术点：自定义Accumulator、随机抽取算法、二次排序、分组取TopN
7、性能调优：普通调优、jvm调优、shuffle调优、算子调优
8、troubleshooting：多个实际生产环境中的，线上复杂报错问题的，剖析和解决方案


页单跳转化率
1、获取任务的日期范围参数
2、查询指定日期范围内的用户访问行为数据
3、获取用户访问行为中，每个session，计算出各个在指定页面流中的页面切片的访问量；实现，页面单跳切片生成以及页面流匹配的算法；session，3->8->7，3->5->7，是不匹配的；
4、计算出符合页面流的各个切片的pv（访问量）
5、针对用户指定的页面流，去计算各个页面单跳切片的转化率
6、将计算结果持久化到数据库中




top10
通过筛选的session，他们访问过的所有品类（）按照品类的点击，评论，转发降序排序，task前10

 * 品类二次排序key
 * 
 * 封装你要进行排序算法需要的几个字段：点击次数、下单次数和支付次数
 * 实现Ordered接口要求的几个方法
 * 
 * 跟其他key相比，如何来判定大于、大于等于、小于、小于等于
 * 
 * 依次使用三个次数进行比较，如果某一个相等，那么就比较下一个
 * 



sortByKey算子，默认情况下，它支持根据int、long等类型来进行排序，但是那样的话，key就只能放一个字段了
所以需要自定义key，作为sortByKey算子的key，自定义key中，封装n个字段，并在key中，自己在指定接口方法中，实现自己的根据多字段的排序算法
然后再使用sortByKey算子进行排序，那么就可以按照我们自己的key，使用多个字段进行排序
1、拿到通过筛选条件的那批session，访问过的所有品类
2、计算出session访问过的所有品类的点击、下单和支付次数，这里可能要跟第一步计算出来的品类进行join
3、自己开发二次排序的key
4、做映射，将品类的点击、下单和支付次数，封装到二次排序key中，作为PairRDD的key
5、使用sortByKey(false)，按照自定义key，进行降序二次排序
6、使用take(10)获取，排序后的前10个品类，就是top10热门品类
7、将top10热门品类，以及每个品类的点击、下单和支付次数，写入MySQL数据库

fileteredid和明细RDD进行join，拿到通过筛选条件的sessionRDD
分别计算出，访问，评论，转发的次数
对通过筛选条件的sessionRDD，分别过滤出访问，评论，转发，然后，map（id,1） reduceByKey
categoryidRDD包含了所有符合条件的session

自定义二次排序key ，要implements order
封装排序算法需要的几个字段
实现ordered接口的几个方法

更其他key相比，如何判定大于，大于等于，小于，小于等于
一次使用三个次数比较






黑名单过滤
、实时计算各batch中的每天各用户对各广告的点击次数

2、使用高性能方式将每天各用户对各广告的点击次数写入MySQL中（更新）

3、使用filter过滤出每天对某个广告点击超过100次的黑名单用户，并写入MySQL中

4、使用transform操作，对每个batch RDD进行处理，都动态加载MySQL中的黑名单生成RDD，然后进行join后，过滤掉batch RDD中的黑名单用户的广告点击行为

5、使用updateStateByKey操作，实时计算每天各省各城市各广告的点击量，并时候更新到MySQL

6、使用transform结合Spark SQL，统计每天各省份top3热门广告：首先以每天各省各城市各广告的点击量数据作为基础，首先统计出每天各省份各广告的点击量；然后启动一个异步子线程，使用Spark SQL动态将数据RDD转换为DataFrame后，注册为临时表；最后使用Spark SQL开窗函数，统计出各省份top3热门的广告，并更新到MySQL中

7、使用window操作，对最近1小时滑动窗口内的数据，计算出各广告各分钟的点击量，并更新到MySQL中

8、实现实时计算程序的HA高可用性

9、对实时计算程序进行性能调优
