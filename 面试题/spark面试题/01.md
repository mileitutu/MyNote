1.Spark master是如何使用zookeeper进行HA的，有哪些元数据保存在zookeeper？
  答：spark通过 spark.deploy.zookeeper.dir 来指定master元数据在zookeeper中的存储位置，这些数据包括worker driver executor application
  standby节点从zookeeper中获取元数据信息来恢复集群运行，才能继续对外提供服务，恢复前不能接受请求
  master切换需要注意2点：
        1.主备master切换过程中，application已经从集群中获取了资源，切换时不影响已经获得的资源，所以在运行时job本身的调度和处理与master没有关系
        2.master切换过程中唯一影响的是不能提交新的job：一方面不能提交新的application给集群，因为只有active master才能接受新的程序的提交请求另一方面，已经在运行的application也不能够因为action操作触发新的job的提交请请求


2.Spark master HA主从切换过程中不影响集群已有作业运行，为什么？
    答：这个说法是不对的，在切换过程中，对已有作业也不是完全不影响，已有作业在执行action算子后不能向集群提交新的job。刨除这个情况，application在运行之前已经向集群申请过资源了，所以不影响


3.spark master的HA如何配置？
    答：1.配置zookeeper
        2.修改spark_env.sh ，添加
        export SPARK_DAEMON_JAVA_OPTS="-Dspark.deploy.recoveryMode=ZOOKEEPER                                   -Dspark.deploy.zookeeper.url=zk01:2181,zk02:2181,zk03:2181 -Dspark.deploy.zookeeper.dir=/spark"
        并将spark_env.sh发送到各个节点
        3.在某个master节点执行./start-all.sh ， 会在此处启动主master。 其他的master备节点，则执行./sbin/start-master.sh
        4.提交程序的时候指定master的时候，要制定三台master
        比如 ./spark-shell --master spark://master01:7077,master02:7077,master03:7077
    
4.driver的功能是什么？
    答：1.application的主进程，具有main函数，是程序的入口，有sparkContext实例
        2.driver生成sparkContext和DAGScheduler，负责向集群申请资源，向master注册信息，负责作业的调度，负责作业的解析，生成stage，调度task到executor上。

5.spark的有几种部署模式？
    1.本地模式
        将spark应用以多线程的方式直接在本地运行，调试用
            1.local：只启动一个executor
            2.local[k]:启动k个executor
            3.local[*]:启动跟cpu数目相同的executor
    2.spark standalone
        分布式部署集群，自带完整的服务，资源管理和任务监控是spark自己监控，这个模式也是其他模式的基础
        包含cluster和client两种运行模式，cluster适合生产，driver运行在集群子节点，具有容错功能，client适合调试，dirver运行在客户端
    3.spark on yarn
        分布式部署集群，资源和任务监控交给yarn，但是目前及支持粗粒度资源分配方式。
        包含cluster和client两种运行模式，cluster适合生产，driver运行在集群子节点，具有容错功能，client适合调试，dirver运行在客户端
    4.spark on mesos
        1.粗粒度模式：每个应用程序的运行环境由若干个executor和一个driver组成，每个application在运行前需要将环境中的资源全部申请好，运行过程中要一直占用着这些资源，即使不使用也占着，最后程序运行结束后，回收资源，这种模式浪费资源
        2.细粒度模式：类似于现在的云计算模式，按需分配
    
6.spark中的worker的作用是什么？
    答：管理当前节点的资源，内存，cpu的使用状况，接收master发送过来的资源分配指令，通过executorrunner启动程序分配任务，worker相当于包工头

7.spark为什么比MapReduce快？
    答：1.基于内存计算，减少低效的磁盘交互
        2.高效的调度算法，基于DAG
        3.容错机制linage

8.hadoop shuffle 和 spark shuffle 的异同？


9.spark有哪些组件？
    答:master-管理集群和节点，不参与计算
       worker-计算节点，进程本身不参与计算，和master汇报
       driver-运行程序的main方法，创建spark cont对象
       spark context-控制整个application的生命周期
       client：用户提交application的入口


10.cache和persist的区别，持久化级别
    答：memory only
        memory and disk
        memory ser
        memory and disk ser
        disk only
        memory only 2
        memory and disk 2


    1.cache只有一个持久化级别memory onlycache调用了persist，persist可以根据情况设置其他的缓存级别
    2.executor执行的时候，内存默认60%做持久化用，40%做task操作

10.简述spark集群搭建步骤
    答：1.准备linux环境，设置集群搭建账号和用户组，设置ssh，关闭防火墙，关闭seLinux，配置host，hostname
        2.配置jdk到环境变量
        3.搭建hadoop集群，如果要做master ha，需要搭建zookeeper集群
        修改hdfs-site.xml,hadoop_env.sh,yarn-site.xml,slaves等配置文件
        4.启动hadoop集群，启动前要格式化namenode
        5.配置spark集群，修改spark-env.xml，slaves等配置文件，拷贝hadoop相关配置到spark conf目录下
        6.启动spark集群。

11.spark-submit的时候如何引入外部jar包 
spark-submit –jars  用逗号分开。这样做的缺点是每次都要指定jar包，如果jar包少的话可以这么做，但是如果多的话会很麻烦。
extraClassPath

提交时在spark-default中设定参数，将所有需要的jar包考到一个文件里，然后在参数中指定该目录就可以了，较上一个方便很多：
spark.executor.extraClassPath=/home/hadoop/wzq_workspace/lib/* spark.driver.extraClassPath=/home/hadoop/wzq_workspace/lib/*

需要注意的是,你要在所有可能运行spark任务的机器上保证该目录存在，并且将jar包考到所有机器上。这样做的好处是提交代码的时候不用再写一长串jar了，缺点是要把所有的jar包都拷一遍。

12.spark服务端口？
8080：spark集群web ui端口，4040：sparkjob监控端口，18080：jobhistory端口

13.Spark job的调度模式？
FIFO:比较stageID，谁小谁先执行；这也很好理解，stageID小的任务一般来说是递归的最底层，是最先提交给调度池的；

FAIR:1.先比较两个stage的 runningtask使用的核数，其实也可以理解为task的数量，谁小谁的优先级高；
    2.比较两个stage的 runningtask 权重，谁的权重大谁先执行；
    3.如果前面都一直，则比较名字了（字符串比较），谁大谁先执行；

14.Stage 的 Task 的数量由什么决定
默认由partition的数量决定
可以自行设置，推荐为executor数量的2-3倍

15.广播变量？
A 任何函数调用
B 是只读的  
C 存储在各个节点 

16.默认的存储级别？
MEMORY_ONLY

17.hive 的元数据存储在 derby 和 MySQL 中有什么区别?
多会话

18.DataFrame 和 RDD 最大的区别?
多了 schema 

19.Master 的 ElectedLeader 事件后做了哪些操作?
直接 ALIVE

20.cache后面能不能接其他算子,它是不是action操作？
答：cache可以接其他算子，但是接了算子之后，起不到缓存应有的效果，因为会重新触发cache。
cache不是action操作

21.reduceByKey是不是action？
不是，很多人都会以为是action，reduce rdd是action

22.数据本地性是在哪个环节确定的？
具体的task运行在那他机器上，dag划分stage的时候确定的

23.4.RDD的弹性表现在哪几点？
1.自动的进行内存和磁盘的存储切换；
2.基于Lingage的高效容错；
3.task如果失败会自动进行特定次数的重试；
4.stage如果失败会自动进行特定次数的重试，而且只会计算失败的分片；
5.checkpoint和persist，数据计算之后持久化缓存
6.数据调度弹性，DAG TASK调度和资源无关
7.数据分片的高度弹性，a.分片很多碎片可以合并成大的，b.par

24.15.RDD创建有哪几种方式？
1.使用程序中的集合创建rdd
2.使用本地文件系统创建rdd
3.使用hdfs创建rdd，
4.基于数据库db创建rdd
5.基于Nosql创建rdd，如hbase
6.基于s3创建rdd，
7.基于数据流，如socket创建rdd

如果只回答了前面三种，是不够的，只能说明你的水平还是入门级的，实践过程中有很多种创建方式。


25.你如何从Kafka中获取数据？
1)基于Receiver的方式
这种方式使用Receiver来获取数据。Receiver是使用Kafka的高层次Consumer API来实现的。receiver从Kafka中获取的数据都是存储在Spark Executor的内存中的，然后Spark Streaming启动的job会去处理那些数据。

2)基于Direct的方式
这种新的不基于Receiver的直接方式，是在Spark 1.3中引入的，从而能够确保更加健壮的机制。替代掉使用Receiver来接收数据后，这种方式会周期性地查询Kafka，来获得每个topic+partition的最新的offset，从而定义每个batch的offset的范围。当处理数据的job启动时，就会使用Kafka的简单consumer api来获取Kafka指定offset范围的数据